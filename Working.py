# -*- coding: utf-8 -*-
"""Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Yheoyzm8e1rA2QoUJYn2BS_Z_xTVvnWZ
"""

!pip install transformers[torch]
!pip install accelerate -U
!pip install datasets
!pip install pandas torch scikit-learn

# train_keyword_extractor.py

import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer, BertForTokenClassification, AdamW
from sklearn.model_selection import train_test_split

class KeywordDataset(Dataset):
    def __init__(self, texts, keywords, tokenizer, max_length):
        self.texts = texts
        self.keywords = keywords
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        keywords = self.keywords[idx].split(', ')

        encoding = self.tokenizer(text, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')
        input_ids = encoding['input_ids'].squeeze()
        attention_mask = encoding['attention_mask'].squeeze()

        labels = torch.zeros_like(input_ids)
        for keyword in keywords:
            keyword_encoding = self.tokenizer(keyword, add_special_tokens=False)
            keyword_ids = keyword_encoding['input_ids']

            for i in range(len(input_ids) - len(keyword_ids) + 1):
                if torch.all(input_ids[i:i+len(keyword_ids)] == torch.tensor(keyword_ids)):
                    labels[i:i+len(keyword_ids)] = 1

        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'labels': labels
        }

def train_model():
    # Load and preprocess the data
    df = pd.read_csv('data.csv')

    # Split the data into train and test sets
    train_texts, test_texts, train_keywords, test_keywords = train_test_split(df['sentence'], df['keywords'], test_size=0.2, random_state=42)

    # Initialize the tokenizer and model
    key_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    key_model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=2)

    # Create datasets and dataloaders
    max_length = 128
    train_dataset = KeywordDataset(train_texts.tolist(), train_keywords.tolist(), key_tokenizer, max_length)
    train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)

    # Training loop
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    key_model.to(device)

    optimizer = AdamW(key_model.parameters(), lr=5e-5)
    num_epochs = 5

    for epoch in range(num_epochs):
        key_model.train()
        for batch in train_dataloader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            outputs = key_model(input_ids, attention_mask=attention_mask, labels=labels)
            loss = outputs.loss

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        print(f"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}")

    # Save the model and tokenizer
    key_model.save_pretrained('keyword_extractor_model')
    key_tokenizer.save_pretrained('keyword_extractor_tokenizer')

    print("Model and tokenizer saved successfully.")

if __name__ == "__main__":
    train_model()

from transformers import GPT2LMHeadModel, GPT2Tokenizer, DataCollatorForLanguageModeling, Trainer, TrainingArguments
from datasets import Dataset
import os

# Load pre-trained model and tokenizer
model_name = "gpt2"
model = GPT2LMHeadModel.from_pretrained(model_name)
tokenizer = GPT2Tokenizer.from_pretrained(model_name)

# Add padding token to the tokenizer
tokenizer.pad_token = tokenizer.eos_token

# Prepare dataset
def prepare_data(sentences, keywords):
    # Combine sentences and keywords for training
    data = [f"Keywords: {', '.join(kws)} Sentence: {sentence}" for kws, sentence in zip(keywords, sentences)]
    return data


sentences = [
    "The quick brown fox jumps over the lazy dog.",
    "She sells seashells by the seashore.",
    "The rain in Spain stays mainly in the plain.",
    "All the villagers came running to drive the wolf away",
    "French East India Company started its first factory at Surat in the year 1668.",
    "The French lost all their bases in India due to the Carnatic wars",
    "The monkeys formed a banana band and performed a circus act to impress the audience.",
    "Gandhi's 1930 salt march left the British with a salty taste.",
    "In 1757, the East India Company won the Battle of Plassey with 'plassey' and flair.",
    "Bhagat Singh's 1931 moustache made him a hero for independence.",
]
keywords = [
    ["quick", "brown", "fox"],
    ["sells", "seashells", "shore"],
    ["rain", "Spain", "plain"],
    ['villagers', 'running', 'drive'] ,
    ["French East India Company", 'Surat', '1668' ],
    ["French","India", "Carnatic"],
    ["monkeys", "bananas", "circus",],
    ["Gandhi", "salt march", "1930",],
    ["East India Company", "bBattle of Plassey", "1757",],
    ["Bhagat Singh", "hero", "1931",],
]

data = prepare_data(sentences, keywords)

# Convert data to a Dataset
dataset_dict = {"text": data}
dataset = Dataset.from_dict(dataset_dict)

# Tokenize the dataset
def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True, max_length=128)

tokenized_datasets = dataset.map(tokenize_function, batched=True)

# Data collator for language modeling
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False
)

# Training arguments
training_args = TrainingArguments(
    output_dir="./results",
    overwrite_output_dir=True,
    num_train_epochs=10,
    per_device_train_batch_size=4,
    save_steps=10_000,
    save_total_limit=2,
)

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=tokenized_datasets,
)

# Train the model
trainer.train()

# Save the model and tokenizer after training
model.save_pretrained("./results")
tokenizer.save_pretrained("./results")

# test_keyword_extractor.py

import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer, BertForTokenClassification
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_recall_fscore_support
import nltk
nltk.download('punkt')
from nltk.tokenize import sent_tokenize

keywords = list()

class KeywordDataset(Dataset):
    def __init__(self, texts, keywords, tokenizer, max_length):
        self.texts = texts
        self.keywords = keywords
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        keywords = self.keywords[idx].split(', ')

        encoding = self.tokenizer(text, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')
        input_ids = encoding['input_ids'].squeeze()
        attention_mask = encoding['attention_mask'].squeeze()

        labels = torch.zeros_like(input_ids)
        for keyword in keywords:
            keyword_encoding = self.tokenizer(keyword, add_special_tokens=False)
            keyword_ids = keyword_encoding['input_ids']

            for i in range(len(input_ids) - len(keyword_ids) + 1):
                if torch.all(input_ids[i:i+len(keyword_ids)] == torch.tensor(keyword_ids)):
                    labels[i:i+len(keyword_ids)] = 1

        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'labels': labels
        }

def test_model():
    # Load the data
    df = pd.read_csv('data.csv')

    # Split the data into train and test sets
    _, test_texts, _, test_keywords = train_test_split(df['sentence'], df['keywords'], test_size=0.2, random_state=42)

    # Load the trained model and tokenizer
    key_tokenizer = BertTokenizer.from_pretrained('keyword_extractor_tokenizer')
    key_model = BertForTokenClassification.from_pretrained('keyword_extractor_model')

    # Create test dataset and dataloader
    max_length = 128
    test_dataset = KeywordDataset(test_texts.tolist(), test_keywords.tolist(), key_tokenizer, max_length)
    test_dataloader = DataLoader(test_dataset, batch_size=8)

    # Evaluation
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    key_model.to(device)
    key_model.eval()

    all_preds = []
    all_labels = []

    with torch.no_grad():
        for batch in test_dataloader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            outputs = key_model(input_ids, attention_mask=attention_mask)
            logits = outputs.logits
            preds = torch.argmax(logits, dim=2)

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    # Calculate metrics
    precision, recall, f1, _ = precision_recall_fscore_support(
        [label for labels in all_labels for label in labels],
        [pred for preds in all_preds for pred in preds],
        average='binary'
    )

    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1 Score: {f1:.4f}")


    # Function to extract keywords from a sentence
    def extract_keywords(sentence):
        encoding = key_tokenizer(sentence, padding='max_length', truncation=True, max_length=max_length, return_tensors='pt')
        input_ids = encoding['input_ids'].to(device)
        attention_mask = encoding['attention_mask'].to(device)

        with torch.no_grad():
            outputs = key_model(input_ids, attention_mask=attention_mask)
            logits = outputs.logits
            preds = torch.argmax(logits, dim=2)

        keywords = []
        current_keyword = []
        for token, pred in zip(key_tokenizer.convert_ids_to_tokens(input_ids[0]), preds[0]):
            if pred == 1:
                current_keyword.append(token)
            elif current_keyword:
                keywords.append(key_tokenizer.convert_tokens_to_string(current_keyword).strip())
                current_keyword = []

        if current_keyword:
            keywords.append(key_tokenizer.convert_tokens_to_string(current_keyword).strip())

        return ', '.join(keywords)


    # Paragraph to be split into sentences
    paragraph = "Shivaji was a Maratha warrior king who founded the Maratha Empire. He was known for his innovative military tactics. Shivaji established a competent and progressive civil rule with well-structured administrative organizations. His legacy is still celebrated in Maharashtra."


    # Split the paragraph into sentences
    sentences = sent_tokenize(paragraph)


    for i, sentence in enumerate(sentences, 1):
      extracted_keywords = extract_keywords(sentence)
      keywords.append(extracted_keywords)
      print(f"Sentence {i}: {extracted_keywords}")


if __name__ == "__main__":
    test_model()

from transformers import GPT2LMHeadModel, GPT2Tokenizer

# Load the fine-tuned model and tokenizer
model_name = "./results"  # Path to your saved model directory
model = GPT2LMHeadModel.from_pretrained(model_name)
tokenizer = GPT2Tokenizer.from_pretrained(model_name)

def prepare_input(keywords):
    input_text = f"Keywords: {', '.join(keywords)} Sentence:"
    input_ids = tokenizer.encode(input_text, return_tensors='pt')
    return input_ids

#keywords = [ 'marathas', 'british', 'the anglo - maratha wars']
input_ids = prepare_input(keywords)

output = model.generate(
    input_ids,
    max_length=50,
    num_return_sequences=1,
    no_repeat_ngram_size=2,
    early_stopping=True,
    temperature=1.0,
    top_k=20,
    top_p=0.95
)

generated_text = tokenizer.decode(output[0], skip_special_tokens=True)

# Extract the generated sentence
prompt_length = len(tokenizer.decode(input_ids[0], skip_special_tokens=True))
generated_sentence = generated_text[prompt_length:]
print(generated_sentence.strip())